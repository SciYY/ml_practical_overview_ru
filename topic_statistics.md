# Статистическое исследование данных

Без знания чисто статистических инструментов исследования данных ни в ML, ни в DS далеко не продвинуться. Однако об этом легко забыть, так как при описании ML алгоритмов свойствам самих данных уделяется мало внимания. Поэтому во время изучения этой темы предлагается не использовать вообще никаких ML инструментов при работе с данными, а сосредоточиться только на статистике.

Некоторые статистические методы базируются на не очень простой математике, поэтому понять их устройство и свойства не всегда бывает просто. Но зато большинство методов можно не реализовывать вручную, а просто делать библиотечные вызовы. При этом основные свойства методов, их ограничения и их смысл всё равно необходимо понимать.

Практический факт: в отличие от нейронных сетей для работы с изображениями, для большинства статистических задач не требуется GPU. Всё работает на CPU с приемлемой скоростью и во многих случаях даже не требует больших объёмов памяти.

## Теоретические темы

Для экономии времени при написании курса в материале ниже даны ссылки на ключевые страницы Википедии, однако во многих местах на этих страницах объяснение или не очень понятное или чересчур подробное для быстрого знакомства. Поэтому эти статьи из Википедии можно воспринимать просто как стартовую точку для поиска дополнительной информации, когда она необходима.

### Краткое напоминание общей теории

В статистике используется статистическое описание процессов. То есть, операции совершаются не с зависимостью X(t), а рассматриваются сразу многие наблюдаемые X при многих моментах времени t. Однако просто смотреть на много значений X(t) неудобно, поэтому вводятся некоторые характеристики, описывающие сразу много значений X(t). Эти характеристики называются статистиками. Например, это среднее значение, плотность распределения и т.п.

Если нужно сравнить несколько процессов (X(t) и Y(t) или X(t) и X(t-tau)), то вводятся статистики и для таких сравнений. Например, это корреляция и ковариация – они упрощённо показывают, насколько два разных процесса похожи друг на друга.

Статистики &ndash; функции, которые на входе принимают один или несколько процессов, а на выходе выдают или число или некоторую функцию от своего параметра. Они удобны тем, что имея всего одну статистику, мы уже можем что-то утверждать о многих значениях X(t), Y(t) и т.п.

Напоминание некоторых терминов из области статистики:
* https://en.wikipedia.org/wiki/Statistic
* https://en.wikipedia.org/wiki/Probability_distribution
* https://en.wikipedia.org/wiki/Correlation_and_dependence
* https://en.wikipedia.org/wiki/Covariance

### Статистическое сравнение процессов

Пожалуй самая частая статистическая задача. Наблюдается много значений X(t), которые "живут своей жизнью" независимо от метода наблюдения. В какой-то момент исследователь что-то меняет, какой-то внешний сложный фактор эксперимента. После этого наблюдение за процессом продолжается - собираются значения Y(t). Если сравнивать прямо, то X(t) и Y(t) всегда будут отличаться, так как всегда есть постоянные внутренние факторы (процессы живут своей жизнью). Один из основных вопросов статистики: _наблюдаемые процессы отличаются только из-за своих внутренних факторов или из-за того изменения, которое производилось между наблюдениями_?

Для ответа на этот вопрос ещё рано оперировать статистиками, так ещё неизвестно, какие статистики могут показать разницу между X(t) и Y(t). Поэтому вводится очень общее понятие: нулевая гипотеза, гласящая, что X(t) и Y(t) не менялись. Все дальнейшие теоретические изыскания для ответа на вопрос выше проводятся в терминах этой гипотезы: её надо или доказать или опровергнуть.

Чтобы доказать или опровергнуть гипотезу (не обязательно нулевую) используются статистические критерии (statistical tests в английском, поэтому и в русском встречается термин "тесты"). Они принимают на вход X(t) и Y(t), а на выходе выдают вероятность _p_. Упрощённо, это вероятность того, что X(t) и Y(t) соответствуют гипотезе. В случае нулевой гипотезы – вероятность того, что X(t) и Y(t) не менялись. То есть, если _p_ очень маленькая для нулевой гипотезы, то есть веские основания думать, что X(t) и Y(t) действительно различаются чем-то большим, чем просто внутренними факторами. К сожалению, высокая _p_ как правило не может служить доказательством того, что ничего в процессах статистически не менялось (это просто гораздо сложнее доказать). Главное, что эта метрика остаётся очень популярной в статистических тестах, и хотя там есть ещё тонкости, можно остановиться только на вышеизложенном смысле.

Чтобы по вероятности _p_ сделать более формальный вывод о процессах, определяется очень низкий порог _alpha_: если _p_ > _alpha_, то ничего нельзя утверждать о разнице X(t) и Y(t), но если _p_ < _alpha_, то это доказывает, что X(t) и Y(t) точно не одно и то же (нулевая гипотеза опровергнута). Полученное таким образом опровержение нулевой гипотезы будем называть статистически значимым. Значимость здесь обозначает уверенность в выводе.

Поняв это, можно воспринимать фразы из научных работ типа "... so according to the measurements, factor XYZ influences on the result (p=0.002) ..." и не мучиться вопросом, что это было за _p_. Тем более, что большинство статистических библиотечных функций будут возвращать как раз это самое _p_.

* Азбука для любой статистической оценки данных:
 * https://en.wikipedia.org/wiki/Null_hypothesis
 * https://en.wikipedia.org/wiki/P-value
 * https://en.wikipedia.org/wiki/Statistical_significance

Статистические тесты бывают разные, и у каждого своя область применения. Когда нужно проверить, менялось что-нибудь или нет, можно просто выбрать подходящий тест из популярных чтобы получить _p_. Разбираться во внутренностях теста не нужно.

* Статистические тесты (или статистические критерии). Можно ограничиться только пониманием изложенных выше принципов:
 * https://en.wikipedia.org/wiki/Test_statistic
* Некоторые популярные варианты получения p:
 * https://en.wikipedia.org/wiki/Student%27s_t-test
 * https://en.wikipedia.org/wiki/F-test
 * https://en.wikipedia.org/wiki/Analysis_of_variance
* Отдельно от остальных тестов стоит упомянуть тест Колмогорова-Смирнова, так как он работает с произвольными распределениями, а большинство остальных тестов только с гауссовыми.
 * https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test
 * Здесь есть информация, что он может доказать только разницу в распределениях, но не то, что они одинаковы:  
   https://stats.stackexchange.com/questions/83163/statistical-test-to-tell-whether-two-samples-are-pulled-from-the-same-population

### Факторный анализ

Кроме статистических тестов, в статистике есть множество других способов анализа, которые могут пригодиться в исследовании данных. Один из таких способов - факторный анализ.

В многих практических задачах часто наблюдается много величин сразу, то есть измеряемое значение X &ndash; вектор, а сам процесс X(t) &ndash; набор векторов. Если вектор X содержит много однотипных значений одинаковой природы, то имеет смысл проверить, не является ли наблюдаемый векторный процесс X(t) всего лишь отражением некоторого векторного процесса Z(t) с меньшей размерностью. То есть, число значений в векторе Z существенно меньше числа значений в векторе X. Другими словами, гипотетически может существовать небольшой набор действительно независимых значений в процессе Z(t), а много значений X(t) являются лишь комбинацией того небольшого набора значений. Оказывается, есть чисто статистические техники поиска Z(t), этим занимается факторный анализ.

Про факторный анализ можно просто запомнить, что это набор техник для нахождения скрытого вектора Z(t). Разбираться в этих техниках не нужно.

* Факторный анализ (нужно понять общую идею):
 * https://en.wikipedia.org/wiki/Factor_analysis

### Анализ главных компонент (Principal Component Analysis)

Близкой областью к факторному анализу является метод главных компонент (PCA). Он позволяет не искать скрытый Z(t), а скомбинировать компоненты X(t) так, что эти комбинации будут максимально независимы. То есть, PCA генерирует из вектора X(t) новый вектор Y(t) такой же размерности, но только все компоненты Y(t) являются комбинациями нескольких компонент X(t).

Это хорошо иллюстрируется графиком гауссовского процесса с разномасштабным распределением из Википедии. Это двумерный гауссовский процесс с независимыми компонентами по длине и ширине. Но в исходных координатах он повёрнут так, что обе составляющие двумерного распределения влияют на значения x(t) и y(t) на осях:  
![Иллюстрация гауссовского процесса](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/220px-GaussianScatterPCA.svg.png "Двумерный гауссовский процесс с разномасштабным распределением")  
PCA находит вектор с двумя новыми координатами (компонентами вектора) `a*x(t) + b*y(t)` и `c*x(t) + d*y(t)` &ndash; эти координаты изображены чёрными стрелками на картинке. Видно, что в этих координатах независимые составляющие процесса разделены &ndash; компоненты вектора совпадают с независимыми составляющими гауссовского процесса.

Компоненты нового вектора не только максимально независимы, но и упорядочены по степени этой "независимости". То есть первая компонента содержит максимум информации о процессе, вторая &ndash; меньше информации, и так далее. Можно отсечь последние компоненты этого вектора, которые не несут важной информации о процессе. Таким образом получается вектор меньшей размерности, но с более важной информацией.

Это важный метод уменьшения количества входной информации в ML модель &ndash; данные можно предобработать PCA и отбросить маловажные компоненты.

* Анализ главных компонент:
 * https://en.wikipedia.org/wiki/Principal_component_analysis

### Предварительная обработка данных

В принципе, тему "предварительная обработка данных" можно тоже отнести к статистическим исследованиям. Но по этой теме слишком много информации, чтобы останавливаться на ней подробно в этом обзоре.

* Для самого общего знакомства можно просмотреть эти источники:
 * http://scikit-learn.org/stable/data_transforms.html
 * http://scikit-learn.org/stable/modules/outlier_detection.html

## Инструменты

* Для общего представления можно почитать первые два ответа тут:
  * https://www.quora.com/What-is-the-relationship-among-NumPy-SciPy-Pandas-and-Scikit-learn-and-when-should-I-use-each-one-of-them
* Информация по отдельным инструментам:
 * https://en.wikipedia.org/wiki/Scikit-learn
 * https://en.wikipedia.org/wiki/SciPy
 * https://en.wikipedia.org/wiki/Pandas_(software)
* Мощная библиотека для построения сложных графиков (или множеств графиков):
 * https://seaborn.pydata.org/

## Практическое задание

Все задачи предлагается выполнять на вот этом датасете, чтобы можно было сравнить результаты:  
https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/

Описание данных из этого датасета:  
https://archive.ics.uci.edu/ml/datasets/Wine+Quality

##### Задачи

* Построить плотности вероятности для каждой из 12 величин на одном графике (используя histograms из matplotlib), чтобы все их сразу можно было охватить взглядом.
* Найти глазами более-менее гауссову величину в данных, которая одинаково выглядит и в данных для красного вина и в данных для белого вина. Назовём эту оставшуюся величину X.
* Найти вероятность, что распределение X было действительно сгенерировано гауссовым (нормальным) процессом
* Проверить, чувствительна статистически ли величина X к тому, красное вино или белое. Для этого нужно сформулировать нулевую гипотезу, подобрать тест, найти _p_ и сделать вывод.
* Проверить, меняются ли остальные распределения величин статистически значимо, если поменять красное вино на белое
* С помощью метода главных компонент найти наиболее значимые линейные величины, влияющие на оценку качества вина
* Для этих величин (их будет меньше 12) построить scatter plot, распределение величин и корреляцию, используя функцию PairGrid из seaborn библиотеки (см. статью из пункта 1 раздела «дополнительное чтение» ниже)
* Если останется время, посмотреть на средства data preprocessing из scikit-learn

## Заблуждения о статистике

**Заблуждение 1.** Встречается достаточно распространённое мнение, что все статистические методы (то есть методы, которые работают с вероятностью, статистиками, распределением вероятности, свойствами этих распределений или статистик) являются линейными. Это в корне неверно.

Статистическое описание просто рассматривает происходящие процессы с другой стороны: не как конкретные значения переменных (детерминированное описание процессов), а через более сложные метрики. Никаких предположений о свойствах рассматриваемого процесса на этом этапе не делается. Более того, иногда какие-то нелинейные эффекты гораздо проще выявить или описать именно статистическими методами (например, любые шумы).

В личном опыте авторов есть следующий пример использования статистики для анализа принципиально нелинейных сигналов. Необходимо было определить, будут ли различные классы видео различаться предобученной глубокой нейронной сетью для анализа видеопотока. Для этого был написан небольшой статистический анализатор, который рассматривал каждый выход сети как случайный процесс (при подаче набора входных сигналов из определённого класса видео). Этим анализатором с помощью статистических тестов можно было оценить, меняется ли выход сети статистически значимо в зависимости от класса видео. При этом анализатор предполагал только статистическую независимость выходов сети друг от друга, но не линейность или нелинейность сигнала на выходе или, например, его гауссовость. В результате получился быстрый предсказатель успеха использования предобученной глубокой сети для классификации видео.

**Заблуждение 2.** Другое распространённое мнение заключается в том, что все эффективные статистические методы работают только для гауссовых распределений, которые редки в реальных задачах. На самом деле в статистике есть набор относительно простых методов, который часто используется для иллюстрации её возможностей. Многие методы из этого набора, действительно, работают исключительно для гауссовых (нормальных) распределений. Но большинство более сложных или менее популярных методов работает и для других типов распределений. Более того, есть методы сведения произвольных распределений к гауссовым.

Без сомнения, нужно внимательно относиться к ограничениям методов, но даже из базового набора:
* среднее значение всегда будет некоторым средним значением (пусть уже не совпадающим с максимумом распределения)
* среднеквадратическое отклонение всегда останется некоторой мерой ширины распределения (но уже не шириной гауссовского "колокола")
* корреляция/ковариация остаются мерой линейной зависимости величин для любых распределений
* PCA расширяется на случай негауссовских распределений
* Критерий Колмогорова-Смирнова работает для любых распределений

Здесь можно коротко отметить, что теоретически могут существовать распределения, для которых не определено среднее, среднеквадратическое отклонение или другие статистики (интегралы для [моментов][momentum_link_avoid_braket_misparsing] или других статистик могут расходиться). Но на практике встретить такие распределения для измеряемых величин достаточно сложно, поэтому в этом обзорном курсе эта тема не рассматривается.

[momentum_link_avoid_braket_misparsing]: https://en.wikipedia.org/wiki/Moment_(mathematics)

**Выводы.** Модели из ML не подменяют собой статистику. С помощью них, в принципе, можно исследовать данные, но в этих способах исследования есть свои сложности по сравнению с обычной статистикой (см. раздел "Анализ данных с помощью моделей" в теме [Не нейросетевые модели](topic_other_models.md)). Нелинейность и негауссовость &ndash; не преимущество ML методов перед статистикой. Если пытаться формулировать кратко, то преимущество ML в том, что в статистике нужно много действий производить вручную на основе анализа и логических выводов, а в ML гораздо большее количество действий автоматизировано. То есть, ML может довольно быстро находить в данных такие связи, которые искать графиками и статистическими тестами было бы гораздо дольше (но всё ещё возможно). Как правило, это преимущество работает на задачах с многими сложно связанными компонентами, что не то же самое, что нелинейность или негауссовость.

Пока ML хорошо обучается, о статистике можно думать мало. Но как только модель перестаёт обучаться, либо перестаёт выдавать желаемый результат, единственный надёжный способ анализа &ndash; статистика. В этом смысле она как отладчик для ML &ndash; может всё то же самое, но пошагово и с объяснением результата. А сами результаты ML, в свою очередь, могут подсказать, какую статистику по данным нужно считать для их отладки.

## Дополнительное чтение

1. Очень хорошая серия статей про ML для разнородных данных (по текущей теме можно прочитать только первую часть):
 * Перевод: https://habr.com/company/nixsolutions/blog/425253/ (есть несколько некритических ошибок перевода)
 * Оригинал: https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420
1. Учебный курс от Kaggle:
 * https://www.kaggle.com/learn/overview
 * и в нём кратко про графики: https://www.kaggle.com/learn/data-visualisation
1. Несколько обзорных статей в Википедии по схожим темам:
 * https://en.wikipedia.org/wiki/Data_analysis
 * https://en.wikipedia.org/wiki/Exploratory_data_analysis
1. Хорошие обзоры от KDnuggets:
 * По текущей теме: https://www.kdnuggets.com/2017/06/7-steps-mastering-data-preparation-python.html
 * Более широкий: https://www.kdnuggets.com/2017/08/42-steps-mastering-data-science.html
1. Уже упоминавшийся в этой теме отличный учебник от scikit-learn:
 * https://scikit-learn.org/stable/tutorial/index.html
 * http://scikit-learn.org/stable/user_guide.html
